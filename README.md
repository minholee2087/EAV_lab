This repository contains two main directories: 'Zero_shot/', which implements the Zero-Shot EEG-based emotion classification model as described in 'Multimodal Joint Representations of EEG and Audio-Vision for Zero-Shot Learning' (under review), and 'Fusion_bottleneck/', which explores feature fusion techniques to enhance multimodal emotion recognition.

## Multimodal Joint Representations of EEG and Audio-Vision for Zero-Shot Learning  
**Overview**
This repository contains the implementation of our multimodal zero-shot learning (ZSL) framework for EEG-based emotion recognition, as presented in the paper (currently under review): "Multimodal Joint Representations of EEG and Audio-Vision for Zero-Shot Learning".
Our approach integrates EEG, audio, and vision modalities to map data into a shared semantic embedding space using contrastive learning. The framework leverages a multimodal audio-vision transformer alongside a shallow EEG transformer to optimize both unimodal and multimodal performance.

## Fusion Bottleneck paper
**Overview**
This repository also contains the implementation of...

## **ğŸ“ Repository Structure**  
â”œâ”€â”€ Zero_shot/                     # Contains the Zero-Shot EEG classification model  
â”‚   â”œâ”€â”€ Transformer_Audio.py       # Transformer model for audio modality  
â”‚   â”œâ”€â”€ Transformer_EEG.py         # Transformer model for EEG modality  
â”‚   â”œâ”€â”€ Transformer_Video.py       # Transformer model for video modality  
â”‚   â”œâ”€â”€ Zeroshot_setting.py        # Configuration and setup for zero-shot learning  
â”‚   â”œâ”€â”€ main_classwise.py          # Main script for class-wise evaluation  
â”‚   â”œâ”€â”€ main_classwise_visualization.py  # Visualization of class-wise results  
â”‚   â”œâ”€â”€ main.py                    # Main executable script for ZSL (runs the chosen settings)  
â”‚   â”œâ”€â”€ utils.py                    # Helper functions  
â”‚   â”œâ”€â”€ config.yaml                 # Model configurations  
â”‚   â”œâ”€â”€ results/                    # Stores model outputs and logs  
â”œâ”€â”€ Fusion_bottleneck/              # Implements feature fusion techniques for emotion recognition  
â”œâ”€â”€ pretrained_models/              # Pre-trained models for users to download and fine-tune  
â”œâ”€â”€ data_processing/                # Scripts for preprocessing the EAV (EEG-Audio-Vision) dataset  
â”‚   â”œâ”€â”€ EEG/                        # Contains EEG data files  
â”‚   â”‚   â”œâ”€â”€ subject_01.pkl          # Example EEG data file  
â”‚   â”œâ”€â”€ Audio/                      # Contains Audio data files  
â”‚   â”œâ”€â”€ Video/                      # Contains Video data files  
â”œâ”€â”€ requirements.txt                # List of dependencies  
â”œâ”€â”€ README.md                       # Project documentation  
â””â”€â”€ LICENSE                         # License information  


## **âš™ï¸ Setup and Installation**  
1. Downloading the Data
  The EAV dataset is required for training and evaluation. Follow the instructions here to request access and download the dataset.

2. Running the Model
  Install the required dependencies:
    pip install -r requirements.txt
  Execute the main script:
    python main.py

3. Updating Pretrained Models
  To update the pretrained models, download them from this link and place them in the 'pretrained_models/' directory.

Citation
If you use this code, please cite this github 'https://github.com/minholee2087/EAV_lab':

Original GitHub Repository for EAV dataset
This project is based on prior research and implementations. You can find the original repository here: 'https://github.com/nubcico/EAV'.
